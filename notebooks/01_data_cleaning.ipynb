{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4511ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/full_text.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d7b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter One\n",
      "\n",
      "Arrest--Conversation with Mrs. Grubach--Then Miss Bürstner\n",
      "\n",
      "\n",
      "Someone must have been telling lies about Josef K., he knew he had done\n",
      "nothing wrong but, one morning, he was arrested. Every day at eight in\n",
      "the morning he was brought his breakfast by Mrs. Grubach's cook--Mrs.\n",
      "Grubach was his landlady--but today she didn't come. That had never\n",
      "happened before. K. waited a little while, looked from his pillow at the\n",
      "old woman who lived opposite and who was watching him with an\n",
      "inquisitiveness quite unusual for her, and finally, both hungry and\n",
      "disconcerted, rang the bell. There was immediately a knock at the door\n",
      "and a man entered. He had never seen the man in this house before. He\n",
      "was slim but firmly built, his clothes were black and close-fitting,\n",
      "with many folds and pockets, buckles and buttons and a belt, all of\n",
      "which gave the impression of being very practical but without making it\n",
      "very clear what they were actually for. \"Who are you?\" asked K., sitting\n",
      "half upright in his\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520dafcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chapters: 10\n",
      "First chapter content:\n",
      "Arrest--Conversation with Mrs. Grubach--Then Miss Bürstner\n",
      "\n",
      "\n",
      "Someone must have been telling lies about Josef K., he knew he had done\n",
      "nothing wrong but, one morning, he was arrested. Every day at eight in\n",
      "the morning he was brought his breakfast by Mrs. Grubach's cook--Mrs.\n",
      "Grubach was his landlady--but today she didn't come. That had never\n",
      "happened before. K. waited a little while, looked from his pillow at the\n",
      "old woman who lived opposite and who was watching him with an\n",
      "inquisitiveness quite u\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split on \"Chapter\" followed by word characters\n",
    "chapters = re.split(r'\\bChapter\\s+[A-Za-z]+\\b', text)\n",
    "\n",
    "# First element before Chapter One will be empty/introduction → remove it\n",
    "chapters = [ch.strip() for ch in chapters if ch.strip()]\n",
    "\n",
    "# Print the first chapter to verify\n",
    "print(\"Number of chapters:\", len(chapters))\n",
    "print(\"First chapter content:\")\n",
    "print(chapters[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6cd676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\drawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\drawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# clean first chapter\n",
    "words = word_tokenize(chapters[0].lower())\n",
    "filtered_words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "\n",
    "#clean all chapters, creating a text file for each chapter\n",
    "\n",
    "import os\n",
    "output_dir = \"../data/cleaned_chapters\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, chapter in enumerate(chapters):\n",
    "    words = word_tokenize(chapter.lower())\n",
    "    filtered_words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "    \n",
    "    # Save each cleaned chapter to a text file\n",
    "    with open(os.path.join(output_dir, f\"chapter_{i+1}_nostopwords.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(filtered_words))       \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
